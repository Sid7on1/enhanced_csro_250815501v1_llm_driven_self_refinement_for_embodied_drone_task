{
  "agent_id": "coder3",
  "task_id": "task_3",
  "files": [
    {
      "name": "memory.py",
      "purpose": "Experience replay and memory",
      "priority": "medium"
    },
    {
      "name": "reward_system.py",
      "purpose": "Reward calculation and shaping",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.RO_2508.15501v1_LLM_Driven_Self_Refinement_for_Embodied_Drone_Task",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.RO_2508.15501v1_LLM-Driven-Self-Refinement-for-Embodied-Drone-Task with content analysis. Detected project type: agent (confidence score: 9 matches).",
    "key_algorithms": [
      "Modification",
      "Retrieval",
      "Forcement",
      "Reasoning",
      "Plan",
      "First",
      "Assessment",
      "Target",
      "Integrated",
      "Learning"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.RO_2508.15501v1_LLM-Driven-Self-Refinement-for-Embodied-Drone-Task.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 1\nLLM-Driven Self-Refinement for Embodied\nDrone Task Planning\nDeyu Zhang, Member, IEEE , Xicheng Zhang, Jiahao Li, Tingting Long, Xunhua Dai, Y ongjian Fu,\nJinrui Zhang, Member, IEEE, Ju Ren, Senior Member, IEEE , Y aoxue Zhang, Senior Member, IEEE\nAbstract \u2014We introduce SRDrone , a novel system designed for self-refinement task planning in industrial-grade embodied drones.\nSRDrone incorporates two key technical contributions: First, it employs a continuous state evaluation methodology to robustly and\naccurately determine task outcomes and provide explanatory feedback. This approach supersedes conventional reliance on\nsingle-frame final-state assessment for continuous, dynamic drone operations. Second, SRDrone implements a hierarchical Behavior\nTree (BT) modification model. This model integrates multi-level BT plan analysis with a constrained strategy space to enable structured\nreflective learning from experience. Experimental results demonstrate that SRDrone achieves a 44.87% improvement in Success Rate\n(SR) over baseline methods. Furthermore, real-world deployment utilizing an experience base optimized through iterative\nself-refinement attains a 96.25% SR. By embedding adaptive task refinement capabilities within an industrial-grade BT planning\nframework, SRDrone effectively integrates the general reasoning intelligence of Large Language Models (LLMs) with the stringent\nphysical execution constraints inherent to embodied drones. Code is available at https://github.com/ZXiiiC/SRDrone .\nIndex Terms \u2014Edge Intelligence, Embodied Drones, Task Planning, Behavior Tree\n\u2726\n1 I NTRODUCTION\nAs the pivotal platform for the low-altitude economy,\ndrones are fundamentally shaping the scale and effective-\nness of this emerging industry [1]. In this context, intelligent\ntask planning is critical for drone deployment, demanding\ncore capabilities such as autonomous decision-making, com-\nplex task comprehension, dynamic environment adaptabil-\nity, and natural human-machine interaction. Key application\nscenarios, e.g., smart grid/pipeline inspection [2], urban\nlogistics delivery [3], and emergency response [4], further\nnecessitate reliable autonomous execution of complex mis-\nsions. Recent breakthroughs in Large Language Models\n(LLMs) and multimodal extensions significantly enhance\nthe cognitive intelligence potential of drone control systems\nby leveraging vast knowledge bases and robust semantic\nreasoning capabilities [5]\u2013[7].\nDespite deploying LLMs for automated planning lever-\naging their powerful reasoning capabilities to generate com-\nplex plan, several critical limitations remain in practical\napplications. (1) High Dependence of Human Expertise.\nOther methods leverage LLMs to generate an initial drone\npath plan. To achieve adaptability in dynamic operational\nenvironments, these approaches rely critically on human\nexperts to perform dynamic online adjustments during\nmission execution [5], [8], [9]. Yet, this dependence on\ncontinuous manual intervention entails prohibitively high\n\u2022Deyu Zhang, Xicheng Zhang, Jiahao Li, Tingting Long, Xunhua\nDai, Yongjian Fu are with School of Computer Science and En-\ngineering, Central South University. Changsha 410083, China. E-\nmails:{zdy876, csu xichengzhang, jiahao li, tingtinglong, dai.xh, fuy-\nongjian }@csu.edu.cn\n\u2022Jinrui Zhang, Ju Ren, Yaoxue Zhang are with the Department of Com-\nputer Science and Technology, Tsinghua University, Beijing 100084,\nChina. E-mails: {jinruizhang, renju, zyx }@tsinghua.edu.cn\n\u2022Corresponding author: Jinrui Zhangoperational costs. Furthermore, expert-driven adjustments\ninherently compromise critical performance attributes, in-\ncluding dynamic responsiveness, persistent task execution,\nand terrain adaptation. This degradation arises from funda-\nmental limitations associated with human factors: cognitive\nprocessing delays, finite operator bandwidth leading to\nsaturation, unstable communication links, and the rigidity\ninherent in transferring expertise to dynamic scenarios. (2)\nInadequate Generalization and Dynamic Adaptability.\nThe predominant approaches employ LLMs to statically\ngenerate Behavior Trees (BTs), exploiting their industrial\ncompatibility to produce executable specifications intended\nfor one-time deployment [10], [11]. However, such static BTs\nfundamentally limited adaptability to Out-of-Distribution\n(OOD) environments. Consequently, when deployed in un-\nfamiliar settings or subjected to unforeseen environmental\nconditions (e.g., unexpected weather), the performance of\nthese static BTs exhibits significant degradation, compromis-\ning the reliability of execution in practical scenarios.\nTo address these limitations, we propose SRDrone , a\nframework designed to enable autonomous and scalable\ndrone task planning without relying on human experts.\nSRDrone employs continuous motion state analysis for\nautonomous performance evaluation of mission execution,\nthereby activating a self-reflective iteration mechanism that\nremoves the requirement for real-time human supervision.\nFurthermore, it utilizes hierarchical behavior tree modifica-\ntion with semantic understanding of behavioral structures.\nThis enables online dynamic correction of LLM-generated\nBTs for robust adaptation in OOD environments.\nHowever, designing the framework faces two key chal-\nlenges: 1) How to achieving reliable self-assessment solely\nbased on execution feedback, without human oversight?\nIt requires the LLMs to interpret complex, multi-modal state\ninformation, accurately detect deviations from the intendedarXiv:2508.15501v1  [cs.RO]  21 Aug 2025\n\n--- Page 2 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 2\ngoal, and provide trustworthy evaluations supported by ex-\nplanatory reflections.Achieving the automation, reliability,\nand robustness of this self-assessment loop constitutes a\nfundamental barrier to realizing genuine task autonomy.\n2) How to convert unstructured textual feedback from\nthe LLMs into precise and structured modifications for\nthe BTs? Establishing a stable mechanism for iterative BT\nrefinement necessitates ensuring logical consistency and\nverifying operational feasibility within this transformation.\nThis involves reconciling the substantial semantic gap be-\ntween informal language descriptions and the formal syntax\nand semantics of tree-based structures.\nTo tackle the above challenges, we integrate several\nnew techniques in SRDrone , each addressing one of the\nabove challenges. 1) Continuous State Evaluation . Onboard\ndrone sensors capture extensive datasets reflecting mission\nexecution. However, these raw sensor streams are typically\nvoluminous and lack explicit semantic interpretability con-\ncerning the underlying drone actions. We posit that extract-\ning the latent action semantics embedded within this data is\ncrucial for effective self-assessment. To achieve this, we pro-\npose the Continuous Motion and Spatial Reasoning (CMSR)\nalgorithm. CMSR explicitly reasons about the drone\u2019s ego-\nmotion trajectory and its spatiotemporal relationships with\nthe surrounding environment. Moreover, the algorithm em-\nploys a selective focus mechanism to analyze only criti-\ncal states within the data stream, significantly mitigating\nthe computational overhead associated with continuous\nreasoning. 2) Hierarchical BT Modification . While LLMs\ngenerate improvement plans through reflection experience,\na significant gap exists between the loosely unstructured\ntextual experience and the syntactically rigorous BTs. Our\ncore insight is that bridging this gap requires structured\nand precise analysis of plan failures to iteratively refine the\nBTs. To achieve this, we design Hierarchical BT Modification to\nperform multi-level semantic analysis of the current BT plan\nto isolate failure points. It then synthesizes precise node-\nlevel modification suggestions by leveraging a constrained\nstrategy space. This transforms abstract LLM reflections into\nexecutable, fine-grade refinement actions.\nWe conduct comprehensive experiments across four\nscenarios: path planning, object searching, obstacle nav-\nigation, and complex composite tasks. In software-based\nand hardware-in-the-loop simulations, SRDrone achieved a\n44.87% higher average Success Rate (SR) compared to state-\nof-the-art baselines. Subsequent real-world deployment on\nphysical drones validated its operational efficacy, attaining\na 96.25% task success rate. This high performance is en-\nabled by the system\u2019s capacity for iterative self-refinement\nof its experience base, initially derived from simulation\ndata. The results demonstrate effective simulation-to-reality\ntransferability, concurrently mitigating the risks and costs\nassociated with drone damage due to real-world mission\nfailures. When evaluating the accuracy of plan-level fail-\nure explanations, our continuous state evaluation method\nachieved 80.07% accuracy. This significantly outperformed\nconventional final-state-checkpoint methods, which yielded\nonly 12.18% accuracy.\nIn summary, our work makes the following main contri-\nbutions:1) We propose SRDrone , to the best of our knowledge,\nthe first framework that enables self-evolving BTs\nfor drone task planning. Our approach overcomes\nadaptability limitations in LLM-based drone sys-\ntems via a closed-loop refinement cycle that facil-\nitates continuous field adaptation without human\nintervention.\n2) We introduce a time-series assessment technique\nfor self-supervised failure diagnosis, and propose\na hierarchical BTs repair mechanism that trans-\nlates unstructured LLM feedback into formal syntax\nmodifications for structural evolution.\n3) We implement and evaluate SRDrone through both\nsimulated experiments and real-world deployment,\ndemonstrating its effectiveness via comparisons\nwith SOTA methods across four representative sce-\nnarios and validating its practical feasibility.\n2 B ACKGROUND AND MOTIVATION\n2.1 Behavior Tree\nBehavior Trees constitute a robust, hierarchical decision-\nmaking architecture prevalent in robotics and AI [11]\u2013[13],\nfundamentally structuring agent behavior as a directed tree\nof interconnected nodes\u2014primarily Control Nodes (e.g.,\nSequence: execute children sequentially until failure; Fall-\nback/Selector: execute children sequentially until success;\nParallel: execute children concurrently) that govern execu-\ntion flow, Execution Nodes (e.g., Action nodes: perform\nstate-changing tasks like movement commands; Condition\nnodes: evaluate true/false predicates about the system or\nenvironment) that define atomic behaviors and checks,\nDecorators to modify child node properties (e.g., looping,\ninverting result), and reusable Subtrees\u2014all orchestrated\nthrough repeated \u201dticking\u201d from the root node, propagating\nticks based on node logic and returning statuses (Success,\nFailure, Running) to dynamically select actions in response\nto real-time changes in a shared data repository (the Black-\nboard).\nIn drone flight control, BTs provide the high-level deci-\nsion logic, decomposing complex missions into manageable\ntasks [14], [15]; for instance, an Action node might command\nflight maneuvers like \u201dmove to waypoint\u201d, while concurrent\nCondition nodes monitor critical factors such as battery\nlevel or obstacle proximity, allowing the BT to dynamically\nadjust the drone\u2019s actions (e.g., triggering a return-to-home\nprocedure if battery is low) by succeeding or failing specific\nbranches based on sensor feedback.\n2.2 Self-Reflection Capability of LLM\nLLM self-reflection denotes the capability to critically eval-\nuate their own outputs based on external feedback and iter-\natively optimize subsequent results [16], [17]. This reflective\nprocess requires two inputs: the model\u2019s prior generated\ncontent or action sequences requiring assessment, such as\ntext passages, code snippets, or decision sequences; and\nexternally provided feedback, including task outcomes, ex-\nplicit error signals, or explicit critiques and guidance [18],\n[19]. The output comprises strategically generated textual\n\n--- Page 3 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 3\ncontent facilitating improvement, such as root-cause anal-\nyses of errors, actionable optimization proposals, and ul-\ntimately yielding revised or enhanced generated content or\nbehaviors through the application of these refined strategies.\nCompared to traditional paradigms like Reinforcement\nLearning (RL) [20]\u2013[22] and Imitation Learning (IL) [23],\nLLM self-reflection offers significant advantages. RL relies\non meticulously designed scalar reward functions, which\noften provide sparse, imprecise signals regarding complex\nbehavioral defects, leading to low sample efficiency and\ndemanding vast interaction data for fine-tuning. IL neces-\nsitates costly, high-quality, comprehensive expert demon-\nstration data, which constrains generalization. In contrast,\nLLM self-reflection leverages natural language feedback\u2014a\nformat inherently richer and more expressive than scalar\nrewards, conveying substantially higher information den-\nsity [24], [25]. Furthermore, the process is driven by the\nLLM\u2019s internal linguistic reasoning, typically enabling rapid\nbehavioral adjustment during inference through iterative\ngeneration and evaluation without requiring computation-\nally expensive weight updates or retraining, thus offering\nenhanced flexibility and computational efficiency [16].\n2.3 A Motivation Study\nThe self-reflection capabilities of LLMs open new opportu-\nnities for enabling truly autonomous and scalable drone task\nplanning. However, during our preliminary experiments,\nwe find two critical issues remain unresolved: (1) LLMs\u2019\ntendency to misjudge task outcomes without human feed-\nback; and (2) the low error correction rates resulting from\nunstructured textual reflection experiences.\n2.3.1 Necessity of Continuous State Evaluation for Plan-\nlevel Failure Detection and Explanation\nUnlike human-involved approaches where experts monitor\nthe entire execution process, existing self-supervised failure\ndetection methods commonly rely on single-frame final\nstate to assess plan-level embodied intelligence task out-\ncomes [17], [26]. However, this paradigm proves unsuitable\nfor plan-level drone missions failure evaluation. Plan-level\nfailures stem from fundamental flaws in the plan\u2019s design,\nsuch that even perfect execution cannot ensure mission\nsuccess. As shown in Figure 1, final-state-only evaluation\ninduces bidirectional diagnostic errors: it systematically\nmisclassifies failed trajectories as successful while misclassi-\nfying correct trajectories as failed.\nIn drone operations, reliance on single-frame terminal\nstates for mission evaluation induces erroneous outcome\njudgments and misleading explanations. This limitation\narises because terminal states merely capture static goal\nachievement, neglecting the continuous spatiotemporal fi-\ndelity inherent in trajectory execution, which demands in-\ntegrated temporal assessment. Consequently, a temporal-\nspatial integrated evaluation framework becomes essential\nfor accurate outcome determination and interpretable fail-\nure analysis. This framework decouples analytical objec-\ntives: (1) Trajectory compliance verification (assessing ad-\nherence to planned motion paths). (2) Geo-referenced po-\nsitioning monitoring (evaluating environment-relative posi-\ntioning). This dual approach significantly enhances evalua-\ntion reliability while ensuring cross-scenario robustness.\nFig. 1: Evaluation results from Final-State-Only method on\nsuccessful and failed trajectories in cross square frame task.\nBlue : Input prompt. Green : Successful trajectory misjudged\nas failures. Red: Failed trajectory misjudged as successful.\nThus, in Section 3.2, we devise a continuous state\nevaluation method that processes onboard sensor streams\nto perform semantic analysis of task-execution processes,\nenabling robust outcome determination and interpretable\nfailure diagnostics.\n2.3.2 Necessity of Structured Error Correction in Behavior\nTree Self-Reflection\nExisting self-reflection frameworks generate unstructured\ntextual experiences that are incompatible with the formal\nsyntax of Behavior Trees, resulting in low correction rate\nfor BT plan flaws. As shown in Table 1, coarse-grained\nreflection mechanisms consistently fail to address hierar-\nchical dependencies during BT repair: our experiments re-\nveal only 39.28% success in detecting missing dependencies\nand 28.6% in correcting invalid control flows across 28\nrepair attempts. These results highlight a critical limitation:\nunstructured textual outputs inherently lack architectural\nconstraint encoding for Behavior Trees. Consequently, cur-\nrent approaches merely patch isolated node actions while\nneglecting inter-node dependencies. Unstructured reflection\nformats are fundamentally incapable of correcting architec-\ntural flaws due to their inherent inability to encode hier-\narchical dependencies. For instance, unstructured text fails\nto formally express missing or invalid node connections-\nfundamental requirements for valid BTs execution.\nTo address this gap, in Section 3.3, we propose Hierar-\nchical BT modification, a structured correction framework\ncomprising two sequential stages. Firstly, hierarchy BT plan\nanalysis systematically identifies structural flaws by analyz-\n\n--- Page 4 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 4\nTABLE 1: Correction rate of existing reflection methods for two typical behavior tree errors\nError Type BT Manifestation Example Correction Rate\nMissing Dependency Absence of precondition nodes Lack actions for searching objectives 39.28%\nInvalid Control Flow Mismatched composite node logic Using Sequence where Fallback required 28.6%\nFig. 2: The overall framework of SRDrone .\ning execution traces and formalizing them as repair con-\nstraints. Secondly, node-level precise modification applies\ntargeted corrections to individual nodes while ensuring\ncompositional integrity. This fine-grained modification ap-\nproach enables atomic action correction for primitive node\nflaws and architectural constraint enforcement to preserve\nthe hierarchical structure of the BT.\n3 SSD RONE\n3.1 Overview\nSRDrone aims to achieve two key goals: (1) robustly eval-\nuate task execution outcomes to detect planning flaws, and\n(2) drive progressive refinement to ensure successful task\ncompletion. Figure 2 illustrates the overall architecture of\nSRDrone , which consists of two main phases:\n(1) During Task Execution Phase , we design a Continuous\nState Evaluation framework which enables reliable outcome\ndetermination and interpretable anomaly attribution for\ndrone operations. This framework initiates with an Action-\nCentric State Capture module that filters critical flight states\nfrom high-frequency drone data streams. Building upon\nthis filtered data, our proposed CMSR algorithm performs\nspatiotemporal semantic extraction, converting multidimen-\nsional temporal sensor data into natural language task nar-\nratives processable by LLMs. The framework culminates in\ntask determination and failure explanation, generating inter-\npretable diagnostic insights to support subsequent planning\noptimization.\n(2) In the Reflective Optimization Phase ,SRDrone identifies\nand resolves planning flaws using a Hierarchical BT Mod-\nification approach. This method facilitates a two-stage re-\nfinement process to generate structural reflective experience\nand guarantee the reliability of refinements. The first stage\nconducts hierarchical analysis of the existing BTs to localize\nerrors across behavioral execution, logical conditions, andplanning structure. Subsequently, error analysis results are\ntransformed into actionable modifications through dual-\nconstraint processing: operational feasibility within hard-\nware/software boundaries and structural validity adhering\nto BT architecture standards. This approach generates pre-\ncise node-level correction specifications for behavioral and\nlogical nodes. Each iteration cycle updates the Experience\nBase with validated corrections, while subsequent planning\niterations leverage this refined knowledge base to optimize\nthe BTs, establishing a closed-loop improvement mechanism\nfor continuous planning enhancement.\n3.2 Continuous State Evaluation\nThe reliability of autonomous drone task refinement de-\npends critically on robust failure detection and interpretable\nerror diagnosis. Existing approaches relying on single-frame\nfinal-state assessment [17], [26] prove fundamentally inad-\nequate for continuous drone operations (Section2.3). This\nlimitation manifests when tasks yield geometrically similar\nterminal states but exhibit diametrically opposed execution\nprocesses . Such critical behavioral distinctions remain un-\ndetectable through snapshot-based evaluation.\nTo enable continuous execution trajectory assessment,\nwe need to address two key problems: (i) High-frequency\nData Streams Drone control systems demand >30 Hz actu-\nation signals, generating high-velocity sensor data streams\nthat rapidly exceed the context window capacity of state-\nof-the-art Large Language Models.(ii) Temporal Data Se-\nmantics Extraction LLMs exhibit inherent limitations in pro-\ncessing continuous numerical time-series data due to their\ndiscrete token-based architecture [27] [28]. This architectural\ngap impedes effective modeling of long-range temporal\ndependencies and complex dynamic patterns essential for\ntask-critical diagnostics [29].\n\n--- Page 5 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 5\nFig. 3: Comparison of different data filtering methods:\nFixed-interval filtering results in insufficient filtering, which\ngenerates redundant data, or excessive filtering, which risks\nlosing critical states. Our Action-Centric Filtering adaptively\nbalances semantic intent preservation with computational\nefficiency, avoiding rigid interval constraints.\n3.2.1 Action-Centric State Filtering\nAutonomous task evaluation requires state capture at\ngranularities that preserve semantic intent while respect-\ning computational constraints.Drone sensors produce high-\nfrequency data streams (typically >30 Hz) that require filter-\ning for downstream processing. For instance, motor control\nsignals in platforms like Pixhawk [30] operate at 100 Hz,\nwhile IMU sensors commonly generate data at 50 Hz.\nHowever, as shown in Figure 3, fixed-interval sampling\nfaces two limitations: 1) Excessive filtering can result in\nthe omission of critical states due to large gaps between\nrecorded samples. 2) Insufficient filtering introduces re-\ndundant data without added semantic value, as raw sensor\nreadings lack explicit behavioral context. For instance, a\n1Hzsample might skip the FlyToCoordinates \u2192Land\ntransition entirely, while 20 Hzsampling captures repetitive\npose updates without distinguishing action semantics. This\ndual challenge, involving the loss of behavioral fidelity at\nlow sampling rates and the presence of semantic ambiguity\nat high sampling rates, underscores the need for action-\naligned state recording to maintain both temporal resolution\nand behavioral intent.\nTo resolve this problem, we introduce action-triggered\nstate recording -capturing execution states siexclusively at\naction completion boundaries. Action-triggered recording\nleverages the inherent characteristics of drone control sys-\ntems. Controllers execute commands as discrete semantic\nunits (e.g., Takeoff ,FlyToCoordinates ), with state tran-\nsitions concentrated at action completion boundaries. These\nboundaries naturally align with kinematically significant\nconfiguration changes where positional and orientational\nshifts manifest most distinctly. By eliminating intra-action\nsampling, this approach achieves data reduction while pre-\nserving all critical state transitions essential for task verifi-\ncation. The resulting triple alignment-semantic, kinematic,\nand computational-ensures captured states reflect mean-\ningful behavioral endpoints rather than arbitrary motion\nintermediates.\nThe process works as follows: (1) Temporal Anchoring:\nRecord state vectors si= (x, y, z, \u03c8 )exclusively at the\ncompletion of each action node ai, where pose (x, y, z )and\nyaw\u03c8, follow NWU convention. (2) Failure-driven Logging:\nFor failed actions, augment siwith failure tag to enable errortracing. (3) Modality-aware Capture:\nDi=(\nsi (motion control nodes)\nsi\u2295 Ei(information capture nodes)(1)\nwhere Eidenotes environmental observations from the\ndrone\u2019s onboard computing platform. Motion control\nnodes (e.g., MoveForward ,FlyToCoordinates ) alter the\ndrone\u2019s own pose, while information capture nodes (e.g.,\nForwardDetectBalloon ) acquire environmental infor-\nmation. The output \u03a0 = ( \u03c01, \u03c02, . . . , \u03c0 n)forms an action-\nannotated state sequence, where each element \u03c0i= (ai,Di)\ncombines an action aiwith its corresponding state-data pair\nDi.\n3.2.2 Spatio-Temporal Semantic Derivation\nBuilding upon the Action-Centric State Capture (Section\n3.2.1), a core challenge in drone task evaluation lies in ana-\nlyzing the execution process, specifically discerning both the\ndrone\u2019s intrinsic motion behaviors and its spatio-temporal\nrelationships with the environment. However, directly uti-\nlizing raw trajectory data for LLM-based analysis faces a\nfundamental barrier: LLMs\u2019 inherent limitations in com-\nprehending continuous numerical time-series data due to\ntheir discrete token-based architecture, as established in\nprior work [27] [28]. To tackle the prevailing limitation,\nour Continuous Motion and Spatial Reasoning (CMSR) al-\ngorithm processes action-annotated data through concur-\nrent semantic derivation streams that extract ego-motion\nbehavior patterns while simultaneously inferring spatial\nenvironmental interactions, with the motion output directly\ninforming spatial reasoning.\nEgo-Motion Semantic Derivation. CMSR initiates by\nextracting the drone\u2019s intrinsic motion characteristics from\nsequential state frames. For each consecutive state pair\n(\u03c0t, \u03c0t+1), the algorithm derives the motion semantics\nthrough spatial-temporal analysis:\n\u20d7Mt=DeriveMotion (\u03c0t, \u03c0t+1) (2)\nwhere \u20d7Mtrepresents a semantic motion vector:\n\u20d7Mt=\uf8ee\n\uf8f0behavior type\ndisplacement\norientation change\uf8f9\n\uf8fb (3)\nThe DeriveMotion process is explicitly designed to\ntranslate low-level trajectory data into high-level, LLM-\ncompatible semantics. To overcome LLMs\u2019 inherent difficul-\nties in interpreting continuous numerical time-series, it en-\ncodes raw motion into structured symbolic representations:\n\u2022behavior type: Classifies the discrete action type\nto provide immediate contextual intent, bypassing\ncomplex motion pattern recognition.\n\u2022displacement: Translates relative position changes\ninto human-readable directional descriptions, replac-\ning dense coordinate sequences with concise spatial\nrelationships.\n\u2022orientation change: Encodes yaw variation as ro-\ntational semantics, abstracting continuous angular\ndata into discrete, meaningful directional shifts.\n\n--- Page 6 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 6\nThese semantic motion primitives are temporally inte-\ngrated to form the ego-trajectory narrative:\nTego=T\u22121M\nt=0\u20d7Mt (4)\nThis reconstruction preserves continuous motion semantics\nessential for understanding execution behaviors, translating\nraw coordinates into textual descriptions:\n\u201dDuring [t2, t3]: FlytoCoordinates, the drone fly to\ncoordinate NWU(1, 2, 1.5)\u201d\n\u201dDuring [t3, t4]: TurnLeft, the drone turn left with\n90\u00b0clockwise rotation\u201d\n\u201dDuring [t4, t5]: MoveForward, the drone move for-\nward 1m to NWU(1, 3, 1.5)\u201d\nEnvironmental Relationship Reasoning. The spatial re-\nlationships between drones and their environments play a\ncritical role in determining mission outcomes. These rela-\ntionships encompass essential safety constraints and opera-\ntional requirements, forming the foundation for evaluating\ntask success. However, current LLMs\u2019 abilities in spatial\nreasoning remain relatively unexplored. [31]. Their inher-\nent limitations in geometric reasoning and fine-grained 3D\ncomputations hinder the derivation of meaningful spatial\nsemantics from coordinate transformations. This bottleneck\nprevents direct application of standard LLM capabilities to\ntasks requiring spatial analysis.\nAlgorithm 1 Continuous Motion and Spatial Reasoning\n(CMSR)\nRequire: Action-annotated state sequence \u03a0 ={\u03c01, ..., \u03c0 T}\nEnsure: Semantic trajectory Tsem\n1:Tego\u2190 \u2205;R \u2190 \u2205 ;O \u2190 \u2205\n2:fort\u21902toTdo\n3: \u20d7Mt\u2190DeriveMotion (\u03c0t\u22121, \u03c0t)// Ego-motion semantic\nderivation between consecutive states\n4:Tego\u2190 T ego\u2295\u20d7Mt // Temporal integration\n5:O \u2190 UpdateEnvironmentState (O, \u03c0t) // Update\nenvironment state with current observation\n6:Rt\u2190 \u2205\n7: for all o\u2208 O do\n8: ro\nt\u2190InferSpatialRelation (\u20d7Mt, o)// Spatial relation\nreasoning with environment object\n9: Rt\u2190 R t\u222a {(o, ro\nt)}\n10: end for\n11:R \u2190 R\u222a{ (t,Rt)}// Store temporal-spatial relationships\n12:end for\n13:Tsem\u2190Align (Tego,R) // Spatio-temporal fusion\n14:return Tsem\nTo this end, we propose a heuristic framework for spatial\nreasoning between drones and environments. We firstly\ninstruct the LLM to maintain a persistent environment state\nrepresentation Othrough heuristic chain-of-thought reason-\ning. At each time step t, the LLM integrates the current\nobservation Etwith temporal context using:\nO \u2190 UpdateEnvironmentState (O, \u03c0t) (5)\nThis cognitive integration process enables Oto adap-\ntively track objects with evolving states while incorporatingentities that re-enter perceptual blind spots, thereby main-\ntaining holistic environment awareness through spatiotem-\nporal reasoning.\nThe updated environment state Othen enables object-\ncentric spatial reasoning by establishing relations between\nego-motion semantics Mtand individual environmental\nentitie o\u2208 O:\nro\nt\u2190InferSpatialRelation (\u20d7Mt, o) (6)\nThe InferSpatialRelation module employs a context-\naware approach using LLM to reason about the drone\u2019s\nspatial relationships based on environmental information.\nIt exploits the inherent characteristics of spatial relation-\nship semantics by modeling the intrinsic properties of\ndrone-environment interactions, generating tuple outputs\nro\nt= (\u03d5intent, \u03d5proximity , \u03d5safety)where each \u03d5represents a\nsemantic descriptor. These outputs enable precise contextual\nreasoning for task evaluation through three key operations:\n1) Navigational Intent : Determines target approach vectors\n(e.g., \u201daligning with landing marker\u201d) by comparing drone\nmotion trends with target coordinates, translating trajec-\ntory patterns into intentional navigation semantics for task\ncompletion assessment. 2) Proximity Awareness : Quantifies\nspatial relationships with key waypoints (e.g., \u201dwithin 2m\nof checkpoint X\u201d) by analyzing distances to critical coordi-\nnates, transforming positional data into actionable proxim-\nity metrics for execution precision evaluation. 3) Collision-\nCentric Safety : Detects obstacle avoidance states (e.g., \u201dcir-\ncumnavigating obstacle B\u201d) through geometric containment\nchecks considering drone dimensions, generating safety se-\nmantics primarily for virtual environment trajectory adjust-\nment.\nThe final semantic trajectory Tsemintegrates both egocen-\ntric motion ( Tego) and environmental spatial relationships\n(R={(t,Rt)}where Rt={(o, ro\nt)}) into LLM-compatible\nnarratives:\n\u201dDuring [t8, t9]: The drone MoveForawrd 1m to\nNWU(-0.93, 1.31, 1.26) while traversing square frame\nat NWU(-0.86, 1.29, 1.18).\u201d\nThis spatio-temporal semantic derivation provides the\nfoundational representation for continuous state evaluation,\nenabling LLM-based interpretation of integrated motion\nand environmental relationship patterns.\n3.3 Hierarchical BT Modification\nSRDrone \u2019s ability to autonomously adapt hinges on dynam-\nically modifying its LLM-generated Behavior Tree. In case\nof uncorrected static BTs operating in Out-of-Distribution\n(OOD) environments, the pre-defined BTs become mis-\naligned with the unpredictable environment, leading to\nsignificant degradation in mission performance and loss of\nautonomy.\nExisting approaches rely on human experts to provide\nprecise adjustments to the Behavior Tree (BT) during execu-\ntion [8]. However, this manual intervention contradicts our\ndesign goal of full autonomy. While leveraging LLMs for\nself-reflective BT correction presents a opportunity to elim-\ninate human dependency, our motivation study (Section\n2.3) reveals a critical limitation: prevailing LLM-based self-\nreflection methods, only use free-form textual outputs, are\n\n--- Page 7 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 7\nFig. 4: The illustration of hierarchical plan analysis.\nunsuited to the rigorous syntactic constraints and complex\nhierarchical structure inherent to Behavior Trees\nTo enable BTs adaptation, this section introduces SR-\nDrone \u2019s hierarchical semantic modification framework for\ncorrecting LLM-generated Behavior Trees in constrained\nstrategy space.\n3.3.1 Hierarchical Plan Analysis\nBased on evaluation results and the current failure plan, the\nframework establishes a hierarchical analysis mechanism\nthat diagnoses BT deficiencies from three parallel perspec-\ntives, as illustrated in Figure 4:\n\u2022Action Layer (\u03b1): At the individual node level, verifies\nwhether action nodes are correctly selected and iden-\ntifies missing critical atomic actions in leaf nodes.\n\u2022Logic Layer (\u03bb): At the node relationship level, as-\nsesses if action nodes are organized with appropriate\ncontrol logic and detects improper logical control\nflows in intermediate nodes.\n\u2022Mission Layer (\u03a0): At the plan-task level, analyzes po-\ntential macro-level misunderstandings between the\ncomposite plan and task requirements.\nThis hierarchical analysis aligns with the inherent archi-\ntecture of Behavior Trees by decomposing their complex\nstructure into semantically distinct layers while preserving\nthe original design principles. The process creates a bidi-\nrectional mapping between the hierarchical structure and\nmission objectives, enabling both fine-grained diagnosis at\nexecution level and high-level understanding of planning\nintent. By maintaining this structural-semantic correspon-\ndence, the analysis not only provides precise localization\nof misalignment sources but also establishes a contextual\nreference for subsequent modifications, ensuring continuity\nwith the original plan\u2019s intent.\n3.3.2 Node-level Precise Modification\nAs demonstrated in Section 2.3, coarse-grained reflective\nexperience exhibit limited effectiveness in rectifying struc-\ntural defects of BT plans. To enhance the executability and\nefficiency of our reflection mechanism, this section proposes\na strategy-space constrained correction framework with\nstructured output formulation.\nwe firstly define a composite strategy space comprising\nAction Space (A) and Logic Space (L). Specifically, the Action\nSpace is determined by drone-specific hardware configura-\ntions - different sensor suites (e.g., LiDAR vs. RGB cameras)\nand computational platforms (e.g., embedded GPUs vs. mi-\ncrocontrollers) yield distinct Aspaces, as they impose phys-\nical constraints on feasible actions. Meanwhile, the Logic\nSpace is defined by BT syntax specifications, encompassingsupported control flow primitives such as sequence nodes,\nselector nodes, and decorator patterns that govern valid\nBT structural transformations. Under this dual-constraint\nframework, we employ a structured reflective experience\nformat for precise BT plan modification:\ne:\u27e8\u03c4, \u03c9\u27e9where \u03c9=\u201d[OPERATION]\u201d| {z }\ncore command+\u201d[RATIONALE]\u201d| {z }\nfunctional justification\n(7)\nSpecifically, the target stratum \u03c4(\u03b1/\u03bb/\u03a0) is determined\nby the preceding hierarchical plan analysis , ensuring align-\nment with the identified plan flaw sources. The \u03c9com-\nponent integrates two essential elements: an imperative\noperation specifying concrete modifications (e.g., \u201dReplace\nSequence with Fallback\u201d) and a functional rationale explain-\ning the modification intent (e.g., \u201dthis allows the drone\nto attempt different strategies\u201d). This integrated approach\nnot only ensures precision but also maintains semantic\nconsistency with mission objectives, thereby establishing a\ncorrection paradigm that effectively balances accuracy with\ninterpretability.\nAlgorithm 2 Hierarchical BT Modification\nRequire: BT init: Initial LLM-generated BT\nEbase: Experience Base\nA,L: Action/Logic constraint spaces\nEnsure: Enew: Reflective experiences for current iteration\n1:Enew\u2190 \u2205 // Initialize experience set\n2:(F\u03b1,F\u03bb,F\u03a0)\u2190HierarchicalAnalysis (BT init) //\nSingle-step layer analysis\n3:for each layer \u03c4\u2208 {\u03b1, \u03bb,\u03a0}do\n4: for all flawf\u2208 F\u03c4do\n5: \u03c9\u2190GenerateOp (f,A,L) +GenerateRation (f)//\nOperation + rationale\n6: Enew\u2190Enew\u222a {\u27e8\u03c4, \u03c9\u27e9} // Structured reflective\nexperience\n7: end for\n8:end for\n9:Ebase\u2190Ebase\u222aEnew // Update experience repository\n10:return Enew // Experiences for BT modification\n3.3.3 Experience Base\nReflective experience Ecan be directly utilized to modify\nthe BT plan. Concurrently, we categorize experiences by task\nunits and preserve each Eobtained through reflection dur-\ning iterative cycles. This approach endows the Experience\nBase with transferability to arbitrary similar tasks, whereas\nindividual-step Eis strictly bound to the plan snapshot of\nits generation iteration. We employ a kNN retriever with the\nall-mpnet-base-v2 model [32] to retrieve kmost relevant task\nexperiences.\nOwing to this architectural design, our methodology not\nonly facilitates dynamic task plan adjustments but also en-\nables acquisition of domain-specific knowledge through of-\nfline iterations. This knowledge can subsequently be trans-\nferred to real-world mission scenarios.\n\n--- Page 8 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 8\nFig. 5: Experimental scenarios of SRDrone . Left: Simulation testing environment employing PX4 Software-in-the-\nLoop/Hardware-in-the-Loop drone models with real-time pose visualization via Unreal Engine; Right: Real-world\ndeployment scenarios replicating identical configurations in both indoor and outdoor environments.\n4 I MPLEMENTATION\n4.1 Overall Implementation\nSRDrone employs an end-to-end system integration ap-\nproach deployed on the real-world drone platform.\nSoftware Implementation: The core system is primarily\nimplemented in C++. Communication with the low-level\nflight controller is handled via MAVROS [33], while the\nbehavior tree framework leverages the BehaviorTree.CPP\nlibrary [34] for execution. Python components manage re-\nquests to remote server-based LLMs and operating the\nonboard vision module using the YOLOv5 object detection\nmodel [35].\nHardware Platforms: The experimental platform is\nZHUOYI FS-J310 multirotor drone, equipped with an\nNVIDIA Jetson Orin NX onboard computer (16GB RAM)\n[36], integrating a 6-core Arm\u00ae Cortex\u00ae-A78AE v8.2 64-\nbit CPU and a 1024-core NVIDIA Ampere architecture GPU\nwith 32 Tensor Cores. The hardware stack included a CUAV\nV5 Nano Autopilot flight controller [37], a Livox Mid-\n360 LiDAR [38], and a FASTLIO localization module [39].\nWe use Intel D435i [40] as vision camera to obtain RGB\nimage and depth estimation. Note that our system remains\nagnostic to the specific methods used for image acquisition\nor depth estimation\u2014compatible alternatives include Edge\nYOLO [41] for object detection, or stereo depth solutions like\nMobiDepth [42] and AnyNet [43].\n4.2 BT Plan Generation\nWe use XML-formatted BTs as our plan representation.\nGiven the strictly defined syntax of BTs, constraining the\nLLM\u2019s output format is essential. While some approaches\nemploy fine-tuning [10] or complex algorithms [44] to gen-\nerate BT plans as task planning results, we propose a\nsimple one-shot method. Our approach remains agnostic\nto the underlying task planning process, focusing solely on\noutput format constraints. This ensures compatibility with\nany reasoning framework.\nWe first provide the XML syntax specification and func-\ntional documentation for each node type, enabling the LLM\nto correctly interpret node capabilities. Subsequently, we\nsupply a complete, simple Behavior Tree one-shot sample\nin the prompt. Empirically, we recommend incorporating\nsyntax examples for blackboard parameter referencing, as\nthis significantly improves reliability.5 E XPERIMENT\n5.1 Experimental Setup\nExperimental Scenarios:\nFigure 5 illustrates our comprehensive experimental\nsetup encompassing both simulation and real-world deploy-\nment. Simulation tests employed PX4 Software-in-the-Loop\n(SIL) and Hardware-in-the-Loop (HIL) environments for\nunmanned system control and flight dynamics modeling.\nReal-time pose (position and orientation) and coordinate\ndata are visualized in 3D using Unreal Engine.\nDuring real-world deployment, experiments are con-\nducted across indoor and outdoor environments while mon-\nitoring onboard computational load. Given the high cost\nof mission failures in practical applications, we directly\nleveraged the validated experience base refined through\nsimulation iterations to verify task success rates. The drone\nmodel FS-J310, as referenced in Section 4.1, is used for all\nfield tests.\nBenchmark: Our experiments include four task types:\nthree typical tasks for drone operations ( Path Planning ,Ob-\nject Searching ,Obstacle Navigation ) and Composite Task that\nbuilds upon these core capabilities. Detailed task instruc-\ntions and scene configurations for each type are provided in\nTable 2.\nBaselines: We select representative frameworks from\ndistinct planning paradigms: static task planning ( ChatFly\n[5]), and self-updating task planning ( LLM-Planner [45] and\nREFLECT [17]). All baselines are obtained from their official\nrepositories and adapted to our experimental setup. The\nstandard LLM-Planner includes replanning; we also eval-\nuate a modified variant, LLM-Planner-HLP , which disables\nreplanning to better assess its static planning capability. No-\ntably, while LLM-Planner utilizes this kNN-based retrieval\napproach, all other baselines employ only one-shot exam-\nples. Since REFLECT lacks a dedicated planner design, we\nuse the same planner described in Section 4.2 as in SRDrone .\nMetrics: For planning performance, we measure Success\nRate ( SR): the ratio of successfully completed user instruc-\ntions. For iterative methods, we allow up to 5 refinement\nsteps. SR is calculated as:\nSR=P5\ni=1N(i)\nsuccess\nNtotal(8)\nwhere N(i)\nsuccess is the number of tasks succeeding at the i-th\niteration, and Ntotalis the total number of tasks.\n\n--- Page 9 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 9\nTABLE 2: Task descriptions and scene configurations.\nType ID Task Instruction Scene Configuration\nPath Planning1 Fly forward first, then fly left to the target point. Center point at (2, 2)\n2 Fly a 2x2 square path around the center point. Center point at (1, 1)\n3 Avoid No-Fly zones and proceed to the target point target (4,1), No-fly zones range in (1,0),(1,3),(2,0),(2,3)\nObject Searching4 Find a balloon in the room The balloon is located at (-2, 3, 1)\n5 Locate the square frame Position of the square frame is randomly initialized\n6 Search for a Landing mark on the floor Landing mark position randomly initialized\nObstacle Navigation7 Fly over the cylinder at 1m height Cylinder is located at (1,1)\n8 Navigate around the rectangular obstacle Obstacle is located at (1,0)\n9 Cross through the square frame Square frame is located at (1, 0)\nComposite Task10 Find and cross through the square frame Position of the square frame is randomly initialized\n11 Avoid No-Fly Zones and land on landing mark No-fly zones range in (1,0),(1,3),(2,0),(2,3)\nTABLE 3: Quantitative comparison of SRDrone and baseline methods. All approaches evaluated without human-in-the-\nloop. SRDrone achieves SOTA performance across all scenarios.\nMethod/Task Path Planning Object Searching Obstacle Navigation Composite Task\nChatFly (2025 TMC) 11.03% 18.46% 21.74% 3.57%\nLLM-Planner-HLP 48.28% 30.43% 26.09% 7.69%\nLLM-Planner (2023 ICCV) 48.28% 43.48% 26.09% 15.79%\nREFLECT (2023 CoRL) 31.03% 52.17% 39.13% 26.32%\nSRDrone (Ours) 84.91% 88.46% 91.67% 80.35%\nFor task evaluation methods, we assess: Detection ( Det):\nratio of detected task failures to total occurrences, Local-\nization ( Loc): precision in identifying the initial position\nof execution errors in the plan, and Explanation ( Exp):\npercentage of failure explanations deemed both reasonable\nand correct by human evaluators.\nsubsectionOverall Performance\n5.1.1 Comparison with Baselines\nTable 3 presents the SRevaluation of SRDrone compared\nagainst SoTA baselines across four benchmark scenarios\nin the simulation environment. SRDrone achieves the best\nperformance across all benchmark scenarios with notable\nmargins of improvement: Path Planning: 84.91% (at least\nimprove 75.9%), Object Searching: 88.46% (at least im-\nprove 69.6%), Obstacle Navigation: 91.67% (at least improve\n134.2%), Composite Task: 80.35% (at least improve 205.3%).\nWe next analyze the underlying mechanisms for SRDrone \u2019s\nsignificantly superior performance by combining insights\nfrom the unique characteristics of different task scenarios.\nWhen handling tasks without explicit failure signals,\nsuch as the Path Planning task requiring assessment of\nintermediate waypoints during drone flight, baseline meth-\nods like LLM-Planner (relying on explicit failure signals)\norREFLECT (employing only final-state verification) prove\nineffective. In contrast, SRDrone with Continuous State Eval-\nuation mechanism enables assessment of dynamic processes,\nproviding robust outcome determination and failure expla-\nnation to drive iterative optimization.\nFor tasks demanding structurally complex logical control\nflows, particularly the Composite Task scenario requiring\nBT syntactic constraints during exceptions like undetectedtarget objects, existing coarse reflection methods accumulate\nunstructured experience, resulting in inefficient iterative\nrefinement. SRDrone \u2019sHierarchical BT Modification approach\novercomes this limitation through precise hierarchical ad-\njustments of BT plans, thus achieving superior refinement\noutcomes.\n5.1.2 Deployment in Real World\nThis section presents experimental validation of the SR-\nDrone framework through real-world deployment in phys-\nical environments, with each task performed ten times in-\ndoors and ten times outdoors. The primary objectives are\nto verify the system\u2019s practical effectiveness in autonomous\naerial tasks and demonstrate its feasibility for deployment\non main-stream drone platforms with limited computational\nresources. Physical testing was exclusively conducted with\nour refined experience base due to unacceptable physical\nrisks associated with trial-and-error learning in real-world\nsettings, where task failures could incur high-cost damages\n(e.g., sensor destruction or personnel injury). To provide\nvisual insights into specific task execution patterns, we\npresent 3D flight path visualizations of Task 2 and Task 10\nin Figure 6.\nAnalysis of experimental results in Table 4 first reveals\nconsistently high success rates across environments, 95%\nindoor and 97.5% outdoor, demonstrating SRDrone \u2019s op-\nerational effectiveness regardless of environment. Detailed\nfailure analysis further establishes that all observed errors\noriginate exclusively at the execution level. Specifically in\nTask 4, the single failure occurred during balloon identifica-\ntion due to perception system limitations, while Task 9 and\n\n--- Page 10 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 10\nTask 10 failures stemmed from environmental misidentifica-\ntion of navigation frames.\n(a) Path planning flight trajectory\n(ID 2)\n(b) Composite task flight trajec-\ntory (ID 10)\nFig. 6: Flight path 3D visualization. (a) The drone executes\na 2m\u00d72m square trajectory around a central point in the XY\nplane. (b) Autonomous navigation demonstration where the\ndrone sequentially detects and passes through two square\nframes positioned along its flight path.\nTABLE 4: Real-world deployment experiments.\nID\u2217Indoor Success/Total (Rate) Outdoor Success/Total (Rate)\n2 10/10 (100%) 10/10 (100%)\n4 10/10 (100%) 9/10 (90%)\n9 9/10 (90%) 10/10 (100%)\n10 9/10 (90%) 10/10 (100%)\n\u2217Detailed descriptions of ID numbers are provided in Table 2.\nTo further validate the practical feasibility of SRDrone ,\nwe conducted comprehensive measurements of key re-\nsource consumption metrics during real-time operation. As\nshown in Figure 7, the system maintains peak CPU over-\nhead below 14%, RAM usage under 110 MB, and total\npower consumption within 1300 mW during task execution.\nThese results are obtained through continuous monitoring\nof ZHUOYI FS-J310 multirotor drone (NVIDIA Jetson Orin\nNX [36]). The low runtime overhead highlights that our\nsystem imposes minimal load on the device, ensuring ease\nof deployment in real-world scenarios.\nThe system\u2019s overhead shows clear variations during\ndifferent stages of task execution. The 12-16 second period\nshows increased CPU and memory usage, corresponding\nto the activation of visual perception modules for scene\nunderstanding. The slightly elevated load in the final phase\nis attributed to the Continuous State Evaluation module,\nwhich involves data stream filtering, state logging opera-\ntions, and communication with cloud-based LLM services.\nThese transient increases remain well within the hardware\u2019s\noperational limits, demonstrating the system\u2019s robustness\nunder dynamic workload variations.\n5.2 Performance Breakdown\nWe next evaluate the performance of the key components of\ntheSRDrone in detail.5.2.1 Plan-level Failure Detection and Explanation\nPlan-level failures occur when a drone completes its action\nsequence but fails to achieve the task goal, making them\nmore complex to diagnose than execution failures. There-\nfore, we compare our proposed Continuous State Evaluation\nmethod with the Final State-based method and an ablated\nversion of our model without CMSR ( Ours w/o CMSR ). All\ncomparisons are conducted on the collected 176 plan-level\nfailures from both simulation and real-world drone task\nexecutions.\nExperimental results in Table 5 indicate that Final State-\nbased method suffers severe limitations in detecting plan-\nlevel failures due to its inability to model task processes,\nresulting in poor performance ( Det=23.08%, Exp=14.72%,\nLoc=12.18%). Ours w/o CMSR leverages task process data\nstreams for evaluation, achieving moderately higher results\n(Det=31.41%, Exp=20.83%, Loc=18.59%) than Final State-\nbased . However, it remains significantly inferior to SRDrone\ndue to the LLM\u2019s limitations in processing time-serial data.\nSRDrone with Continuous State Evaluation framework\nachieves superior failure analysis quality ( Det=90.38%,\nExp=85.26%, Loc=80.07%). This result underscores the ne-\ncessity of process state assessment for drone task scenarios.\nFinal-state evaluations fail to capture systemic planning\nissues, whereas our approach effectively addresses these\nchallenges by leveraging the semantic continuity of the\ntask process. Thus, our system provides a robust solution\nfor plan-level failure detection and explanation in drone\nmission scenarios.\nTABLE 5: Plan-Level failure evaluation comparison.\nMethods Det Exp Loc\nOurs w/o CMSR 31.41 20.83 18.59\nFinal State-based 23.08 14.72 12.18\nSRDrone (Ours) 90.38 85.26 80.07\n5.2.2 Iterative Refinement Process Comparison\nFigure 8 illustrates the iterative refinement process, featur-\ning our SRDrone system alongside the REFLECT and LLM-\nPlanner baselines, both equipped with dynamic planning\nadjustment capabilities. We also include an ablation study\ngroup (\u201d w/o Hierarchical Modification \u201d) which removes the\nentire Hierarchical Modification module, directly utilizing\nfailure explanations from the Evaluation module for BT\nModification.\nThe experimental results establish SRDrone \u2019s significant\nsuperiority over REFLECT under identical initial plans.\nNotably, while LLM-Planner achieves higher initial success\nrates due to its advanced planner, SRDrone surpasses this\nbaseline within two refinement iterations. As optimization\ncycles progress, SRDrone demonstrates increasingly dom-\ninant performance over the ablation variant w/o Hierarchy\nModification , conclusively validating the efficacy of our Hier-\narchical BT Modification module.\nWhile both REFLECT and the ablation variant w/o Hi-\nerarchical Modification achieve comparable result due to\nnear-ubiquitous fault detection, the ablation\u2019s slight per-\nformance advantage originates directly from SRDrone \u2019s\n\n--- Page 11 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 11\n(a) CPU Workload\n (b) Total Power\n (c) RAM Usage\nFig. 7: Performance of SRDrone on physical drone platform.\nTABLE 6: Quantitative comparison of SRDrone and Human-Guide variants.\nMethod/Task Path Planning Object Searching Obstacle Navigation Composite Task\nGeneral-Guide 86.82% 80.21% 84.19% 68.44%\nExpert-Guide 90.33% 92.14% 93.87% 82.50%\nSRDrone 84.91% 88.46% 91.67% 80.35%\n(a)Path Planning\n (b) Object Searching\n(c)Obstacle Navigation\n (d)Composite Task\nFig. 8: Iterative refinement process comparison.\nhigher-quality task process evaluation. This superior assess-\nment capability enables more precise failure diagnosis even\nwithout hierarchical modification, demonstrating clear ad-\nvancement over REFLECT \u2019s failure explanation mechanism.\nFor tasks requiring architectural management of multi-\nlayered BT structures, our Hierarchical BT Modification en-\nables fine-grained plan adjustments. The Composite Task sce-\nnario exemplifies this challenge, demanding concurrent syn-\ntactic satisfaction and exception handling within complex\nBT architectures. In such environments, SRDrone maintains\nsignificant performance advantages over the w/o Hierarchical\nModification ablation. The widening performance gap with\nincreasing plan complexity conclusively demonstrates the\ncritical role of hierarchical refinement mechanisms.5.2.3 Human-in-the-Loop Performance Comparison\nIn addition to automated approaches, we introduce two\nhuman-in-the-loop configurations that substitute the sys-\ntem\u2019s Continuous State Evaluation and Hierarchical BT Mod-\nification modules. In these configurations, humans directly\noversee task execution and provide iterative plan refinement\nthrough natural language feedback. In the Expert-Guide\ncondition, participants possess expertise in AI and drone\nsystems, whereas General-Guide participants have no formal\ntraining in computer science or robotics. The quantitative\ncomparison results are summarized in Table 6.\nSRDrone achieves consistent superiority over General-\nGuide across all scenarios except for Path Planning . This\nadvantage primarily stems from SRDrone \u2019s structural re-\nflective experience, which enabling precise localization of\nBT plan flaws. In contrast, General-Guide is hindered by the\narbitrary mapping between high-level human preferences\nand low-level adjustments, often resulting in infeasible\ncorrections that exceed the drones\u2019 physical constraints.\nIn the Path Planning scenario, SRDrone exhibits slightly\nlower effectiveness compared to General-Guide , a limitation\nattributable to the inherent challenges LLMs face in mathe-\nmatical reasoning, even when compared to general users.\nWhile SRDrone shows a slight performance gap com-\npared to Expert-Guide , this discrepancy primarily stems from\nthe latter\u2019s ability to holistically integrate visual, spatial, and\nsensor data rather than relying on text-based abstractions,\nenabling more accurate error localization. Additionally, hu-\nman experts surpass LLMs in geometric reasoning and\ntrajectory optimization, allowing them to directly detect\nspatial constraint violations without requiring the iterative\nrefinements necessary for autonomous systems. Notably,\nthe gap between SRDrone and Expert-Guide narrows most\nsignificantly in the Composite Task scenario. This observation\nsuggests that SRDrone \u2019sHierarchical BT Modification mecha-\nnism provides increasing benefits as task complexity grows,\nleveraging structured reflective experience to systematically\nresolve flaws across multiple levels of abstraction, thereby\noutperforming verbal feedback-based approaches.\n\n--- Page 12 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 12\n6 R ELATED WORK\nStatic Planning Methods . Current approaches leverage\nLarge Language Models (LLMs) to generate structured task\nplans using formalisms such as Python programs [46] and\nBehavior Trees [8], [44]. These methods benefit from the\nexpressiveness and industrial compatibility of such repre-\nsentations. Techniques including fine-tuning [10] and Rein-\nforcement Learning [22] are further employed to enhance\nLLMs\u2019 embodied physical reasoning capabilities, ultimately\naiming to increase the task success rate of generated plans.\nHowever, these static planning methods exhibit limitations\nin dynamic adaptation. They produce fixed executable spec-\nifications which lack online adaptation mechanisms. This\ncharacteristic results in performance degradation when en-\ncountering OOD scenarios such as unfamiliar settings or\nunexpected environmental conditions. Consequently, the\ninherent rigidity of static planners fundamentally constrains\nautonomous agents\u2019 capacity to respond to real-world un-\ncertainties.\nTo solve these limitations, SRDrone introduces the Hier-\narchy Plan Modification mechanism, which enables precise\nadjustments to BT plans during execution and overcomes\nthe rigidity of static planners. Furthermore, by leveraging\nthe LLM\u2019s self-reflection capabilities, our method does not\nrequire pre-training on specific scenarios, thereby enhancing\nscalability and robustness in out-of-distribution environ-\nments.\nHuman Robot Collaboration . Recent studies have ex-\nplored diverse strategies for integrating human expertise\ninto LLM-based task planning autonomous systems. For\ninstance, Chen et al.\u2019s work [5], where human experts define\nreplanning conditions for dynamic environments, and the\nsystem leverages an LLM to trigger robust task re-planning\nwhen these conditions are met. Parakh et al.\u2019s work [9]\nintroduces a framework where linguistic instructions from\nexperts guide the hierarchical construction of complex skills\nfrom low-level primitives, enabling task adaptation. Mean-\nwhile, Liu et al.\u2019s work [47] adopts a tele-operation system\nthat bypasses autonomous algorithms entirely, allowing ex-\nperts to directly control robots for improved manipulation\nprecision. In contrast, Zhao et al.\u2019s work [48] demonstrates a\nlearning paradigm where policies are distilled from minimal\nexpert demonstrations, achieving superhuman performance\nthrough the DOUBLE EXPLORATION algorithm. Zha et\nal.\u2019s work [49] further advances this field by proposing\nDROC, an LLM-based system capable of processing arbi-\ntrary language feedback to extract generalizable knowledge\nfrom corrections. Despite these innovations, a critical limi-\ntation persists: over-reliance on human expertise. Most ap-\nproaches require users with domain specialized knowledge,\nrendering them inaccessible to general users [50], [51]. This\ndependency not only contradicts the core design objective\nof drones\u2014to reduce human workload\u2014but also hinders\nscalability in real-world applications.\nInSRDrone , we design the Continuous State Evaluation\nframework, which automates drone mission monitoring by\nreplacing human experts with a continuous state evaluation\npipeline. Specifically, the system automatically identifies\nmission-critical events and infers high-level task objectives\nthrough heuristic reasoning that leverages the foundationalreasoning capabilities of LLMs, without requiring scenario-\nspecific training. This not only enhances scalability in OOD\nenvironments but also enables real-time, self-explanatory\ndecision-making through natural language insights.\n7 D ISCUSSION\nAdaptability of SRDrone :SRDrone enables truly au-\ntonomous task execution without human expertise through\ndual adaptive mechanisms. The framework leverages fun-\ndamental behavior tree architectural principles to imple-\nment Hierarchy Fine-grained BT Modification, ensuring uni-\nversal compatibility with any behavior tree-formatted plans\nregardless of task requirements. This capability provides\ninherent task-agnostic adaptability. Furthermore, it utilizes\ncontinuous state evaluation addressing core motion char-\nacteristics of drone operations, delivering agent-agnostic\nperformance for heterogeneous drone fleets. These syner-\ngistic mechanisms drive a self-reflective iteration process\nthat eliminates human supervision dependency. SRDrone\nalso maintains broad LLM compatibility with models like\nGPT [52], Gemini [53], DeepSeek [54] and Qwen [55] by\nconstraining only planner output formats while preserving\nnative reasoning processes, enabling flexible deployment\nacross diverse autonomous systems.\nThe worst performance case: SRDrone demonstrates\nstrong performance in managing Plan-level tasks but lacks\nthe ability to provide detailed error diagnostics at the\nExecution-level . For example, in the Object Searching task,\ntheDetect action is often paired with TurnLeft orTurnRight\nto adjust the field of view. If the turning angle exceeds\noptimal limits, the drone may trigger the Detect action\nbefore stabilizing, causing blurred images and subsequent\ndetection failures. To mitigate this, we introduce an Action-\nCentric State Filtering mechanism that discards images cap-\ntured during unsuccessful Detect actions, effectively re-\nducing communication overhead and computational load.\nHowever, this design inherently limits the system\u2019s ability\nto provide fine-grained failure explanations in such edge\ncases.\nFuture Works: SRDrone currently relies on cloud-\nbased LLMs requiring server communication, utilizes SOTA\nLLMs\u2019 context capabilities without dedicated experience\nbase optimization by feeding raw historical data directly\ninto prompts, and operates without visual-language inte-\ngration. Future enhancements will focus on: (1) Developing\nedge-compatible LLMs for on-device deployment to elim-\ninate cloud dependency; this aims to remove reliance on\nstable communication links and enable operational capabil-\nities in adversarial environments. (2) Implementing struc-\ntured experience compression and retrieval mechanisms to\ntranscend current context-window limitations; this enhance-\nment will strengthen complex task learning capabilities\nthrough optimized knowledge distillation, while improving\ngeneralization performance across similar yet distinct mis-\nsion scenarios. (3) Integrating Vision-Language Models to\nenable cross-modal scene understanding for complex spatial\ndecision-making scenarios; this integration will enhance\nspatial reasoning accuracy through multi-modal perception\nfusion, particularly for navigation and object interaction\ntasks in dynamic environments.\n\n--- Page 13 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 13\n8 C ONCLUSION\nThis paper presents SRDrone , a self-refining framework\nfor autonomous drone task planning and execution. The\napproach combines continuous state evaluation during mis-\nsion runtime with hierarchical, fine-grained behavior tree\n(BT) plan modification, enabling adaptive and robust perfor-\nmance. SRDrone achieves state-of-the-art results, delivering\nthe highest success rates across diverse operational scenar-\nios and exhibiting substantial performance gains in com-\nplex composite environments compared to existing meth-\nods. Furthermore, the effectiveness of SRDrone is validated\nthrough extensive real-world experiments on physical drone\nplatforms, demonstrating its practicality and reliability un-\nder realistic conditions.\nREFERENCES\n[1] X. Huang, \u201cThe small-drone revolution is coming\u2014scientists need\nto ensure it will be safe,\u201d Nature , vol. 637, no. 8044, pp. 29\u201330, 2025.\n[2] N. Tang, N. Landas, Y. R. Rodrigues, and M. R. Monteiro, \u201cIn-\ndustry state-of-art and opportunities for the use of drones in\nsmart grids inspections,\u201d in 2024 International Symposium on Power\nElectronics, Electrical Drives, Automation and Motion (SPEEDAM) .\nIEEE, 2024, pp. 327\u2013331.\n[3] A. Serrano-Hernandez, A. Ballano, and J. Faulin, \u201cSelecting freight\ntransportation modes in last-mile urban distribution in pamplona\n(spain): An option for drone delivery in smart cities,\u201d Energies ,\nvol. 14, no. 16, p. 4748, 2021.\n[4] A. Agrawal, S. J. Abraham, B. Burger, C. Christine, L. Fraser, J. M.\nHoeksema, S. Hwang, E. Travnik, S. Kumar, W. Scheirer et al. ,\n\u201cThe next generation of human-drone partnerships: Co-designing\nan emergency response system,\u201d in Proceedings of the 2020 CHI\nConference on Human Factors in Computing Systems , 2020, pp. 1\u201313.\n[5] G. Chen, X. Yu, N. Ling, and L. Zhong, \u201cChatfly: Low-latency\ndrone planning with large language models,\u201d IEEE Transactions on\nMobile Computing , 2025.\n[6] B. Piggott, S. Patil, G. Feng, I. Odat, R. Mukherjee, B. Dhar-\nmalingam, and A. Liu, \u201cNet-gpt: A llm-empowered man-in-the-\nmiddle chatbot for unmanned aerial vehicle,\u201d in Proceedings of the\nEighth ACM/IEEE Symposium on Edge Computing , 2023, pp. 287\u2013\n293.\n[7] Y. Wang, J. Farooq, H. Ghazzai, and G. Setti, \u201cMulti-uav place-\nment for integrated access and backhauling using llm-driven op-\ntimization,\u201d in 2025 IEEE Wireless Communications and Networking\nConference (WCNC) . IEEE, 2025, pp. 1\u20136.\n[8] J. Ao, F. Wu, Y. Wu, A. Swikir, and S. Haddadin, \u201cLlm as bt-\nplanner: Leveraging llms for behavior tree generation in robot task\nplanning,\u201d arXiv preprint arXiv:2409.10444 , 2024.\n[9] M. Parakh, A. Fong, A. Simeonov, T. Chen, A. Gupta, and\nP . Agrawal, \u201cLifelong robot learning with human assisted lan-\nguage planners,\u201d in 2024 IEEE International Conference on Robotics\nand Automation (ICRA) . IEEE, 2024, pp. 523\u2013529.\n[10] R. A. Izzo, G. Bardaro, and M. Matteucci, \u201cBtgenbot: Behavior\ntree generation for robotic tasks with lightweight llms,\u201d in 2024\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS) . IEEE, 2024, pp. 9684\u20139690.\n[11] X. Chen, Y. Cai, Y. Mao, M. Li, W. Yang, W. Xu, and J. Wang,\n\u201cIntegrating intent understanding and optimal behavior planning\nfor behavior tree generation from human instructions,\u201d arXiv\npreprint arXiv:2405.07474 , 2024.\n[12] R. Ghzouli, T. Berger, E. B. Johnsen, A. Wasowski, and S. Dragule,\n\u201cBehavior trees and state machines in robotics applications,\u201d IEEE\nTransactions on Software Engineering , vol. 49, no. 9, pp. 4243\u20134267,\n2023.\n[13] L. Ruifeng, W. Jiasheng, Z. Haolong, and T. Mengfan, \u201cResearch\nprogress and application of behavior tree technology,\u201d in 2019 6th\nInternational Conference on Behavioral, Economic and Socio-Cultural\nComputing (BESC) . IEEE, 2019, pp. 1\u20134.\n[14] R. Ghzouli, T. Berger, E. B. Johnsen, S. Dragule, and A. Wasowski,\n\u201cBehavior trees in action: a study of robotics applications,\u201d in\nProceedings of the 13th ACM SIGPLAN international conference on\nsoftware language engineering , 2020, pp. 196\u2013209.[15] P . Ogren, \u201cIncreasing modularity of uav control systems using\ncomputer game behavior trees,\u201d in Aiaa guidance, navigation, and\ncontrol conference , 2012, p. 4458.\n[16] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao,\n\u201cReflexion: Language agents with verbal reinforcement learning,\u201d\nAdvances in Neural Information Processing Systems , vol. 36, pp. 8634\u2013\n8652, 2023.\n[17] Z. Liu, A. Bahety, and S. Song, \u201cReflect: Summarizing robot\nexperiences for failure explanation and correction,\u201d arXiv preprint\narXiv:2306.15724 , 2023.\n[18] M. Wadhwa, X. Zhao, J. J. Li, and G. Durrett, \u201cLearning to\nrefine with fine-grained natural language feedback,\u201d in Findings\nof the Association for Computational Linguistics: EMNLP 2024 ,\nY. Al-Onaizan, M. Bansal, and Y.-N. Chen, Eds. Miami, Florida,\nUSA: Association for Computational Linguistics, Nov. 2024, pp.\n12 281\u201312 308. [Online]. Available: https://aclanthology.org/2024.\nfindings-emnlp.716/\n[19] Y. Chen, J. Arkin, Y. Hao, Y. Zhang, N. Roy, and C. Fan,\n\u201cPRompt optimization in multi-step tasks (PROMST): Integrating\nhuman feedback and heuristic-based sampling,\u201d in Proceedings\nof the 2024 Conference on Empirical Methods in Natural\nLanguage Processing , Y. Al-Onaizan, M. Bansal, and Y.-N.\nChen, Eds. Miami, Florida, USA: Association for Computational\nLinguistics, Nov. 2024, pp. 3859\u20133920. [Online]. Available:\nhttps://aclanthology.org/2024.emnlp-main.226/\n[20] F. Peiyuan, Y. He, G. Huang, Y. Lin, H. Zhang, Y. Zhang, and\nH. Li, \u201cAgile: A novel reinforcement learning framework of llm\nagents,\u201d Advances in Neural Information Processing Systems , vol. 37,\npp. 5244\u20135284, 2024.\n[21] M. Rita, F. Strub, R. Chaabouni, P . Michel, E. Dupoux, and\nO. Pietquin, \u201cCountering reward over-optimization in LLM\nwith demonstration-guided reinforcement learning,\u201d in Findings\nof the Association for Computational Linguistics: ACL 2024 , L.-W.\nKu, A. Martins, and V . Srikumar, Eds. Bangkok, Thailand:\nAssociation for Computational Linguistics, Aug. 2024, pp.\n12 447\u201312 472. [Online]. Available: https://aclanthology.org/2024.\nfindings-acl.740/\n[22] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David,\nC. Finn, C. Fu, K. Gopalakrishnan, K. Hausman et al. , \u201cDo as i can,\nnot as i say: Grounding language in robotic affordances,\u201d arXiv\npreprint arXiv:2204.01691 , 2022.\n[23] J. Sun, Q. Zhang, Y. Duan, X. Jiang, C. Cheng, and R. Xu, \u201cPrompt,\nplan, perform: Llm-based humanoid control via quantized imita-\ntion learning,\u201d in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA) . IEEE, 2024, pp. 16 236\u201316 242.\n[24] T. R. Sumers, M. K. Ho, R. D. Hawkins, K. Narasimhan, and\nT. L. Griffiths, \u201cLearning rewards from linguistic feedback,\u201d in\nProceedings of the AAAI Conference on Artificial Intelligence , vol. 35,\nno. 7, 2021, pp. 6002\u20136010.\n[25] Z. Wu, Y. Hu, W. Shi, N. Dziri, A. Suhr, P . Ammanabrolu, N. A.\nSmith, M. Ostendorf, and H. Hajishirzi, \u201cFine-grained human\nfeedback gives better rewards for language model training,\u201d Ad-\nvances in Neural Information Processing Systems , vol. 36, pp. 59 008\u2013\n59 033, 2023.\n[26] D. Das and S. Chernova, \u201cSemantic-based explainable ai: Leverag-\ning semantic scene graphs and pairwise ranking to explain robot\nfailures,\u201d in 2021 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS) . IEEE, 2021, pp. 3034\u20133041.\n[27] C. Chang, W.-Y. Wang, W.-C. Peng, and T.-F. Chen, \u201cLlm4ts:\nAligning pre-trained llms as data-efficient time-series forecasters,\u201d\nACM Transactions on Intelligent Systems and Technology , vol. 16,\nno. 3, pp. 1\u201320, 2025.\n[28] M. Jin, S. Wang, L. Ma, Z. Chu, J. Y. Zhang, X. Shi, P .-Y. Chen,\nY. Liang, Y.-F. Li, S. Pan, and Q. Wen, \u201cTime-LLM: Time series\nforecasting by reprogramming large language models,\u201d in Interna-\ntional Conference on Learning Representations (ICLR) , 2024.\n[29] A. Zeng, M. Chen, L. Zhang, and Q. Xu, \u201cAre transformers\neffective for time series forecasting?\u201d in Proceedings of the AAAI\nconference on artificial intelligence , vol. 37, no. 9, 2023, pp. 11 121\u2013\n11 128.\n[30] A. Thiercelin. (2025) pixhawk. [Online]. Available: https:\n//pixhawk.org/\n[31] W. Wu, S. Mao, Y. Zhang, Y. Xia, L. Dong, L. Cui, and F. Wei,\n\u201cMind\u2019s eye of llms: visualization-of-thought elicits spatial rea-\nsoning in large language models,\u201d Advances in Neural Information\nProcessing Systems , vol. 37, pp. 90 277\u201390 317, 2024.\n\n--- Page 14 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 14\n[32] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, \u201cMpnet: Masked\nand permuted pre-training for language understanding,\u201d Advances\nin neural information processing systems , vol. 33, pp. 16 857\u201316 867,\n2020.\n[33] MAVLink. (2025) Mavros. [Online]. Available: https://github.\ncom/mavlink/mavros\n[34] BehaviorTree. (2025) Behaviortree.cpp. [Online]. Available: https:\n//www.behaviortree.dev/\n[35] Ultralytics. (2025) Yolov5. [Online]. Available: https://github.\ncom/ultralytics/yolov5\n[36] NVIDIA. (2022) Jetson orin nx 16gb. [Online]. Avail-\nable: https://www.nvidia.com/en-us/autonomous-machines/\nembedded-systems/jetson-orin/\n[37] CUAV . (2020) Cuav v5 nano autopilot. [Online]. Available:\nhttps://doc.cuav.net/controller/v5-autopilot/en/v5-nano.html\n[38] livox. (2025) mid-360. [Online]. Available: https://www.livoxtech.\ncom/cn/mid-360\n[39] Hong Kong University MaRS Lab. (2024) Fast-lio: Fast\nlidar-inertial odometry. [Online]. Available: https://github.com/\nhku-mars/FAST LIO\n[40] intel. (2025) d435i. [Online]. Available: https://www.\nintelrealsense.com/depth-camera-d435i/\n[41] S. Liang, H. Wu, L. Zhen, Q. Hua, S. Garg, G. Kaddoum, M. M.\nHassan, and K. Yu, \u201cEdge yolo: Real-time intelligent object de-\ntection system based on edge-cloud cooperation in autonomous\nvehicles,\u201d IEEE Transactions on Intelligent Transportation Systems ,\nvol. 23, no. 12, pp. 25 345\u201325 360, 2022.\n[42] J. Zhang, H. Yang, J. Ren, D. Zhang, B. He, T. Cao, Y. Li, Y. Zhang,\nand Y. Liu, \u201cMobidepth: Real-time depth estimation using on-\ndevice dual cameras,\u201d in Proceedings of the 28th Annual International\nConference on Mobile Computing And Networking , 2022, pp. 528\u2013541.\n[43] Y. Wang, Z. Lai, G. Huang, B. H. Wang, L. Van Der Maaten,\nM. Campbell, and K. Q. Weinberger, \u201cAnytime stereo image depth\nestimation on mobile devices,\u201d arXiv preprint arXiv:1810.11408 ,\n2018.\n[44] X. Chen, Y. Cai, Y. Mao, M. Li, W. Yang, W. Xu, and\nJ. Wang, \u201cIntegrating intent understanding and optimal behavior\nplanning for behavior tree generation from human instructions,\u201d\ninProceedings of the Thirty-Third International Joint Conference on\nArtificial Intelligence , ser. IJCAI \u201924, 2024. [Online]. Available:\nhttps://doi.org/10.24963/ijcai.2024/755\n[45] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and\nY. Su, \u201cLlm-planner: Few-shot grounded planning for embodied\nagents with large language models,\u201d in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , October 2023.\n[46] J. Liang, W. Huang, F. Xia, P . Xu, K. Hausman, B. Ichter, P . Florence,\nand A. Zeng, \u201cCode as policies: Language model programs for\nembodied control,\u201d in 2023 IEEE International Conference on Robotics\nand Automation (ICRA) . IEEE, 2023, pp. 9493\u20139500.\n[47] H. Liu, Y. Zhu, K. Kato, A. Tsukahara, I. Kondo, T. Aoyama,\nand Y. Hasegawa, \u201cEnhancing the llm-based robot manipulation\nthrough human-robot collaboration,\u201d IEEE Robotics and Automation\nLetters , 2024.\n[48] H. Zhao, X. Yu, D. M. Bossens, I. Tsang, and Q. Gu, \u201cBeyond-\nexpert performance with limited demonstrations: Efficient\nimitation learning with double exploration,\u201d in The Thirteenth\nInternational Conference on Learning Representations , 2025. [Online].\nAvailable: https://openreview.net/forum?id=FviefuxmeW\n[49] L. Zha, Y. Cui, L.-H. Lin, M. Kwon, M. G. Arenas, A. Zeng, F. Xia,\nand D. Sadigh, \u201cDistilling and retrieving generalizable knowledge\nfor robot manipulation via language corrections,\u201d in 2024 IEEE\nInternational Conference on Robotics and Automation (ICRA) . IEEE,\n2024, pp. 15 172\u201315 179.\n[50] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von\nArx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al. , \u201cOn\nthe opportunities and risks of foundation models,\u201d arXiv preprint\narXiv:2108.07258 , 2021.\n[51] V . Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai,\nA. Chaffin, A. Stiegler, A. Raja, M. Dey, M. S. Bari, C. Xu,\nU. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani,\nN. Nayak, D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica,\nS. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj,\nJ. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan, T. L.\nScao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush, \u201cMultitask\nprompted training enables zero-shot task generalization,\u201d in\nInternational Conference on Learning Representations , 2022. [Online].\nAvailable: https://openreview.net/forum?id=9Vrb9D0WI4[52] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L.\nAleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al. ,\n\u201cGpt-4 technical report,\u201d arXiv preprint arXiv:2303.08774 , 2023.\n[53] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut,\nJ. Schalkwyk, A. M. Dai, A. Hauth, K. Millican et al. , \u201cGemini:\na family of highly capable multimodal models,\u201d arXiv preprint\narXiv:2312.11805 , 2023.\n[54] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu,\nS. Ma, P . Wang, X. Bi et al. , \u201cDeepseek-r1: Incentivizing reason-\ning capability in llms via reinforcement learning,\u201d arXiv preprint\narXiv:2501.12948 , 2025.\n[55] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge,\nY. Han, F. Huang et al. , \u201cQwen technical report,\u201d arXiv preprint\narXiv:2309.16609 , 2023.\nDeyu Zhang[S\u201914, M\u201916] received the B.Sc. de-\ngree in communication engineering from PLA\nInformation Engineering University, Zhengzhou,\nChina, in 2005, and the M.Sc. degree in com-\nmunication engineering and Ph.D. degree in\ncomputer science from Central South University,\nChangsha, China, in 2012 and 2016, respec-\ntively. He is currently an Associate Professor with\nthe School of Computer Science and Technol-\nogy. His current research interests include mo-\nbile system optimization, edge computing, and\nstochastic optimization. He is a member of the ACM, IEEE, and CCF .\nXicheng Zhang received his B.Sc. degree in\nComputer Science from Central South Univer-\nsity, China, in 2023. He is currently in the second\nyear of his Ph.D studies in Computer Science\nat the same institution. His research interests\ninclude mobile computing and embodied intelli-\ngence.\nJiahao Li received his B.Sc. degree in Computer\nScience from Central South University, China, in\n2024. He is currently in the second year of his\nM.Sc. studies in Computer Science at the same\ninstitution. His research interests include mobile\ncomputing and edge intelligence.\nTingting Long received her B.Sc. degree in\nComputer Science from Central South Univer-\nsity, China, in 2023. She is currently in the\nsecond year of her M.Sc. studies in Computer\nScience at the same institution. Her research\ninterests include mobile computing and edge in-\ntelligence.\n\n--- Page 15 ---\nIEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. XX, XXX 2021 15\nXunhua Dai received the B.S., M.S., and Ph.D.\ndegrees in control science and engineering\nat Beihang University, Beijing, China, in 2013,\n2016, and 2020, respectively.\nSince 2020, he has been an Associate Pro-\nfessor with Central South University in computer\nscience and engineering, where he is currently\nwith the School of Computer Science and Engi-\nneering. His main research interests include re-\nliable intelligent control, safety assessment, and\ndesign optimization of unmanned aerial robotics.\nYongjian Fu received the B.Sc. in Computer\nScience from Central South University in 2021,\nChina. He is currently working toward a Ph.D.\ndegree in Computer Science at the School of\nComputer Science and Engineering from Cen-\ntral South University. His research interests in-\nclude wireless sensing, mobile computing, and\nInternet-of-Things.\nJinrui Zhang[S\u201919, M\u201923] received his B.Sc.,\nM.Sc., Ph.D. degrees all in computer science,\nfrom Central South University, China, in 2016,\n2018 and 2023, respectively. Currently, he is\na postdoctoral researcher with the Department\nof Computer Science and Technology, Tsinghua\nUniversity, China. He has worked as a visit-\ning scholar at Seoul National University, Korea.\nHis research interests include mobile computing,\nedge intelligence, and computer vision. He is a\nmember of the ACM, IEEE, and CCF .\nJu Ren[M\u201916,SM\u201921] received his B.Sc., M.Sc.,\nPh.D. degrees all in computer science, from\nCentral South University, China, in 2009, 2012\nand 2016, respectively. Currently, he is an asso-\nciate professor with the Department of Computer\nScience and Technology, Tsinghua University,\nBeijing, China. Prior to joining Tsinghua, he was\na professor with the School of Computer Sci-\nence and Engineering, Central South University,\nChangsha, China. his research interests include\nInternet-of-Things, edge computing, distributed\n& embedded AI, and operating system. He is a senior member of the\nIEEE and a member of the ACM. He was recognized as a highly cited\nresearcher by Clarivate in 2020-2022.\nYaoxue Zhang [M\u201917, SM\u201918] received the\nB.Sc. degree from the Northwest Institute of\nTelecommunication Engineering, Xi\u2019an, China, in\n1982, and the Ph.D. degree in computer net-\nworking from Tohoku University, Sendai, Japan,\nin 1989. He is currently a professor with the\nDepartment of Computer Science and Technol-\nogy, Tsinghua University, China. His research\ninterests include computer networking, operat-\ning systems, and transparent computing. He\nhas published more than 200 papers on peer-\nreviewed IEEE/ACM journals and conferences. He is the editor-in-chief\nof Chinese Journal of Electronics and a fellow of the Chinese Academy\nof Engineering.",
  "project_dir": "artifacts/projects/enhanced_cs.RO_2508.15501v1_LLM_Driven_Self_Refinement_for_Embodied_Drone_Task",
  "communication_dir": "artifacts/projects/enhanced_cs.RO_2508.15501v1_LLM_Driven_Self_Refinement_for_Embodied_Drone_Task/.agent_comm",
  "assigned_at": "2025-08-24T20:46:36.444768",
  "status": "assigned"
}